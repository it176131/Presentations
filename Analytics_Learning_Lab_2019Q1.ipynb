{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [The 21 Best Spring Break Destinations in America Gallery](https://www.thedailymeal.com/travel/best-spring-break-destinations-in-america-gallery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.thedailymeal.com/travel/best-spring-break-destinations-in-america-gallery\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Requesting the Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Close the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is Soup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "str?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "BeautifulSoup?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Soup can `find` things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#main-content\n",
      "/\n",
      "https://www.facebook.com/TheDailyMeal\n",
      "http://www.pinterest.com/thedailymeal/\n",
      "https://twitter.com/thedailymeal\n",
      "https://www.youtube.com/user/TheDailyMealVideo\n",
      "http://instagram.com/thedailymeal\n",
      "https://www.linkedin.com/company/the-daily-meal\n",
      "http://thedailymeal.tumblr.com/\n",
      "https://www.thedailymeal.com/rss.xml\n",
      "/newsletter/subscriptions\n",
      "/\n",
      "/\n",
      "/community\n",
      "/ccn/culinary-content-network\n",
      "/user\n",
      "/cook\n",
      "/cook\n",
      "/cook/quick-easy\n",
      "/cook/breakfast\n",
      "/cook/special-occasions\n",
      "/cook/cooking-ideas\n",
      "/cook/daily-meals-guide-baking\n",
      "/cook/chefs-cookbooks\n",
      "/cook/seasonal\n",
      "/cook/back-to-school\n",
      "/cook/desserts\n",
      "/cook/how-to-do-it\n",
      "/cook/kitchen-tools\n",
      "/cook/say-hello-to-summer\n",
      "/cook/grilling-and-barbecue-essentials\n",
      "/cook/ice-cream-time\n",
      "/cook/cookies\n",
      "/best-recipes\n",
      "/best-recipes\n",
      "/best-recipes/cake\n",
      "/best-recipes/chicken\n",
      "/best-recipes/crock-pot\n",
      "/best-recipes/ground-beef\n",
      "/best-recipes/healthy\n",
      "/best-recipes/pasta\n",
      "/best-recipes/pork-chop\n",
      "/best-recipes/quinoa\n",
      "/best-recipes/salad\n",
      "/best-recipes/salmon\n",
      "/best-recipes/soup\n",
      "/best-recipes/vegetarian\n",
      "/drink\n",
      "/drink\n",
      "/wine\n",
      "/drink/cocktails\n",
      "/drink/where-to-drink\n",
      "/drink/beer\n",
      "/drink/coffee-and-tea\n",
      "/eat\n",
      "/eat\n",
      "/eat/restaurants\n",
      "/eat/chefs-and-personalities\n",
      "/casual-eats\n",
      "/eat/sandwiches\n",
      "/eat/products\n",
      "/eat/food-for-thought\n",
      "/healthy-eating\n",
      "/healthy-eating\n",
      "/healthy-eating/healthy-kitchen\n",
      "/healthy-eating/swaps\n",
      "/healthy-eating/mind-body\n",
      "/healthy-eating/weight-loss\n",
      "/entertain\n",
      "/entertain\n",
      "/entertain/etiquette\n",
      "/entertain/weddings-and-wedding-cakes\n",
      "/holiday/family\n",
      "/entertain/hosting-tips\n",
      "/entertain/setting-the-scene\n",
      "/entertain/football\n",
      "/entertain/celebrity-bites\n",
      "/holidays\n",
      "/holidays\n",
      "/holiday/new-years\n",
      "/valentines-day\n",
      "/holiday/celebrate-st-patricks-day\n",
      "/passover-lander\n",
      "/holiday/celebrate-easter-with-the-daily-meal\n",
      "/holiday/cinco-de-mayo\n",
      "/holiday/graduation\n",
      "/celebrate-mothers-day\n",
      "/holiday/fathers-day\n",
      "/celebrate-fourth-of-july\n",
      "/holiday/halloween\n",
      "/holiday/thanksgiving\n",
      "/holiday/celebrating-hannukah\n",
      "/christmas\n",
      "/travel\n",
      "/travel\n",
      "/travel/trip-tips\n",
      "/travel/culinary-vacations\n",
      "/travel/global-cuisine\n",
      "/travel/festivals-and-events\n",
      "/video\n",
      "/lists\n",
      "/\n",
      "/travel\n",
      "/travel/trip-tips\n",
      "https://www.thedailymeal.com/users/syjil-ashraf\n",
      "https://www.thedailymeal.com/free-tagging-cuisine/climate-change\n",
      "https://www.thedailymeal.com/free-tagging-cuisine/spring-break\n",
      "https://www.thedailymeal.com/travel/12-accessible-international-spring-break-destinations\n",
      "https://www.thedailymeal.com/travel/10-absolutely-wildest-party-destinations-spring-break\n",
      "https://www.thedailymeal.com/travel/10-best-beaches-spring-break-getaway\n",
      "https://www.thedailymeal.com/eat/best-fun-and-casual-restaurants-11-top-spring-break-destinations\n",
      "https://www.thedailymeal.com/eat/dining-guide-caesars-atlantic-city-hotels\n",
      "https://www.thedailymeal.com/bethany-beach-boardwalk\n",
      "https://www.thedailymeal.com/travel/10-popular-american-beaches-where-you-can-actually-legally-drink-alcohol\n",
      "https://www.thedailymeal.com/11-chic-and-affordable-beach-escapes\n",
      "https://www.thedailymeal.com/worlds-30-most-glamorous-camping-destinations-for-food\n",
      "https://www.thedailymeal.com/travel/ultimate-weekend-getaway-cape-cod\n",
      "https://www.thedailymeal.com/travel/slideshow-prettiest-town-every-state\n",
      "https://www.thedailymeal.com/eat/memorable-dining-hamptons-after-labor-day\n",
      "https://www.thedailymeal.com/travel/hamptons-guide-best-bars-and-restaurants-slideshow\n",
      "https://www.thedailymeal.com/travel/slideshow-10-absolutely-wildest-party-destinations-spring-break\n",
      "http://www.thedailymeal.com/las-vegas-restaurant-guide\n",
      "https://www.thedailymeal.com/travel/101-best-weekend-getaways-america-gallery-0/slide-72\n",
      "https://www.thedailymeal.com/eat/america-s-25-best-fudge-shops-0/slide-10\n",
      "https://www.thedailymeal.com/travel/fresh-taste-martha-s-vineyard\n",
      "https://www.thedailymeal.com/travel/20000-orchids-bloom-miami-beach\n",
      "https://www.thedailymeal.com/long-weekend-myrtle-beach\n",
      "https://www.thedailymeal.com/travel/america-s-40-best-seafood-shacks-0/slide-30\n",
      "https://www.thedailymeal.com/travel/how-do-napa-weekend-your-welcome\n",
      "https://www.thedailymeal.com/napa-valleys-best-wineries-view\n",
      "https://www.thedailymeal.com/travel/top-places-you-need-to-visit-2018-gallery\n",
      "http://www.thedailymeal.com/travel/where-eat-spring-break-youve-got-eat-sometime-slideshow/slide-2\n",
      "https://www.thedailymeal.com/point-pleasant-beach-boardwalk?venue_update=1\n",
      "https://www.thedailymeal.com/californias-catalina-island-cozy-and-romantic-winter-destination\n",
      "https://www.thedailymeal.com/travel/slideshow-10-absolutely-wildest-party-destinations-spring-break/slide-9\n",
      "https://www.thedailymeal.com/travel/worlds-best-beaches-gallery\n",
      "/free-tagging-cuisine/trip-tips\n",
      "/free-tagging-cuisine/travel\n",
      "/travel/12-reasons-cruises-are-best-choice-spring-break-vacation\n",
      "/travel/12-reasons-cruises-are-best-choice-spring-break-vacation\n",
      "/healthy-eating/30-quick-and-easy-recipes-use-spring-superfoods\n",
      "/healthy-eating/30-quick-and-easy-recipes-use-spring-superfoods\n",
      "/cook/how-spring-clean-your-kitchen-10-easy-steps\n",
      "/cook/how-spring-clean-your-kitchen-10-easy-steps\n",
      "/\n",
      "/cook\n",
      "/best-recipes\n",
      "/drink\n",
      "/eat\n",
      "/healthy-eating\n",
      "/entertain\n",
      "/holidays\n",
      "/travel\n",
      "/community\n",
      "/video\n",
      "/lists\n",
      "/about-us\n",
      "/contact-us\n",
      "/privacy-policy\n",
      "/legal\n",
      "http://www.aboutads.info/consumers/\n",
      "/recipes-glossary\n",
      "/newsletter/subscriptions\n",
      "https://bestreviews.com/kitchen?utm_source=tdm&utm_medium=footer&utm_campaign=kitchen\n",
      "https://www.facebook.com/TheDailyMeal\n",
      "https://twitter.com/thedailymeal\n",
      "https://plus.google.com/+Thedailymeal\n",
      "http://www.pinterest.com/thedailymeal/\n",
      "http://instagram.com/thedailymeal\n",
      "https://www.thedailymeal.com/rss.xml\n",
      "/\n",
      "http://www.tribpub.com/\n",
      "http://www.theactivetimes.com/\n"
     ]
    }
   ],
   "source": [
    "# find all hyperlinks\n",
    "# 'a' stands for anchor tag, href stands for Hypertext REFrence\n",
    "for link in soup.find_all(name='a', href=True):\n",
    "    print(link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the List of Vacation Spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h2>The 21 Best Spring Break Destinations in America</h2>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# soup.find returns the first child (see docs)\n",
    "soup.find(name='h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the the result\n",
    "result = soup.find(name='h2')\n",
    "\n",
    "# display the type of the `result`\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 21 Best Spring Break Destinations in America'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access the text in `result`\n",
    "result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h2>The 21 Best Spring Break Destinations in America</h2>,\n",
       " <h2>Atlantic City</h2>,\n",
       " <h2>Bethany Beach, Del.</h2>,\n",
       " <h2>Cannon Beach, Ore.</h2>,\n",
       " <h2>Cape Cod</h2>,\n",
       " <h2>Cape May, N.J.</h2>,\n",
       " <h2>The Hamptons</h2>,\n",
       " <h2>Kennebunkport, Maine</h2>,\n",
       " <h2>Key West</h2>,\n",
       " <h2>Lake Geneva, Wis.</h2>,\n",
       " <h2>Las Vegas</h2>,\n",
       " <h2>Mackinac Island, Mich.</h2>,\n",
       " <h2>Martha’s Vineyard</h2>,\n",
       " <h2>Miami Beach</h2>,\n",
       " <h2>Myrtle Beach</h2>,\n",
       " <h2>Nantucket, Mass.</h2>,\n",
       " <h2>Napa Valley</h2>,\n",
       " <h2>New Orleans</h2>,\n",
       " <h2>Panama City Beach, Fla.</h2>,\n",
       " <h2>Point Pleasant Beach, N.J.</h2>,\n",
       " <h2>Santa Catalina Island, Calif.</h2>,\n",
       " <h2>South Padre Island, Texas</h2>,\n",
       " <h2 class=\"label-above block-title\">More From The Daily Meal</h2>,\n",
       " <h2><a href=\"/travel/12-reasons-cruises-are-best-choice-spring-break-vacation\">12 Reasons Cruises Are the Best Choice for Spring Break Vacation</a></h2>,\n",
       " <h2><a href=\"/healthy-eating/30-quick-and-easy-recipes-use-spring-superfoods\">30 Quick and Easy Recipes That Use Spring Superfoods</a></h2>,\n",
       " <h2><a href=\"/cook/how-spring-clean-your-kitchen-10-easy-steps\">How to Spring-Clean Your Kitchen in 10 Easy Steps</a></h2>,\n",
       " <h2 class=\"block-title\">\n",
       " <span>Footer Navigation</span>\n",
       " </h2>,\n",
       " <h2 class=\"block-title\">\n",
       " <span>About Navigation</span>\n",
       " </h2>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all locations in the html\n",
    "soup.find_all(name='h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all results\n",
    "results = soup.find_all(name='h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the type of `results`\n",
    "type(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 21 Best Spring Break Destinations in America\n",
      "Atlantic City\n",
      "Bethany Beach, Del.\n",
      "Cannon Beach, Ore.\n",
      "Cape Cod\n",
      "Cape May, N.J.\n",
      "The Hamptons\n",
      "Kennebunkport, Maine\n",
      "Key West\n",
      "Lake Geneva, Wis.\n",
      "Las Vegas\n",
      "Mackinac Island, Mich.\n",
      "Martha’s Vineyard\n",
      "Miami Beach\n",
      "Myrtle Beach\n",
      "Nantucket, Mass.\n",
      "Napa Valley\n",
      "New Orleans\n",
      "Panama City Beach, Fla.\n",
      "Point Pleasant Beach, N.J.\n",
      "Santa Catalina Island, Calif.\n",
      "South Padre Island, Texas\n",
      "More From The Daily Meal\n",
      "12 Reasons Cruises Are the Best Choice for Spring Break Vacation\n",
      "30 Quick and Easy Recipes That Use Spring Superfoods\n",
      "How to Spring-Clean Your Kitchen in 10 Easy Steps\n",
      "\n",
      "Footer Navigation\n",
      "\n",
      "\n",
      "About Navigation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the text for each result in `results`\n",
    "for result in results:\n",
    "    print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of the result.text's\n",
    "# define result_texts as an empty list\n",
    "result_texts = []\n",
    "# loop through the results...\n",
    "for result in results:\n",
    "    # and append them to the result_texts list\n",
    "    result_texts.append(result.text)\n",
    "\n",
    "# show result_texts\n",
    "result_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the location names without the index\n",
    "for location in result_texts:\n",
    "    print(location[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem: What if I had a list of 10 locations?\n",
    "# I wouldn't be able to use my logic above because it would keep an extra space in front of the name.\n",
    "# use `split` to remove the index\n",
    "for location in result_texts:\n",
    "    print(location.split()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the location names without indices in a new list\n",
    "no_index = []\n",
    "for location in result_texts:\n",
    "    no_index.append(location.split()[1:])\n",
    "\n",
    "# show `no_index`\n",
    "no_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `no_index` is a list of lists\n",
    "# want to join the strings in each list together to get a list of strings\n",
    "for l in no_index:\n",
    "    print(''.join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it worked, but the join didn't put a space between the words\n",
    "# try it again, but include a space\n",
    "for l in no_index:\n",
    "    print(' '.join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# much better\n",
    "# let's save these locations in another list\n",
    "locations = []\n",
    "for l in no_index:\n",
    "    locations.append(' '.join(l))\n",
    "\n",
    "# show locations\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the `len`gth of locations (should == 8)\n",
    "len(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that seemed like a lot of work just to extract the names from our results...\n",
    "# well here's a one-liner\n",
    "[' '.join(i.text.split()[1:]) for i in soup.find_all(name='strong')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bit of code is called [list comprehension](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions). It's very handy and is often faster than standard for-loops (when n is large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1000\n",
    "result_texts = []\n",
    "for i in soup.find_all(name='strong'):\n",
    "    result_texts.append(' '.join(i.text.split()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1000\n",
    "[' '.join(i.text.split()[1:]) for i in soup.find_all(name='strong')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a precautionary side-note, don't use list comprehension if you don't understand it or the loop is too complex. You can get unexpected results, especially when attempting multiple loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Information on the Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia url\n",
    "wiki_url = \"https://en.wikipedia.org/wiki/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the locations\n",
    "for l in locations:\n",
    "    # add the wiki url to the location name and replace spaces with underscores\n",
    "    print(f\"{wiki_url}{l.replace(' ', '_')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape each page and store in a dictionary\n",
    "# define empty dictionary\n",
    "location_wiki = {}\n",
    "# loop through the locations...\n",
    "for l in locations:\n",
    "    # request the page from wikipedia\n",
    "    r = requests.get(f\"{wiki_url}{l}\")\n",
    "    # make the soup\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    # close the request\n",
    "    r.close()\n",
    "    # store the soup in the dictionary\n",
    "    location_wiki[l] = soup\n",
    "    # wait 2 seconds before scraping again (this respects the robots.txt... and keeps us from getting blocked by wikipedia)\n",
    "    sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we've scraped the wiki pages, let's store some information in a dataframe using `pandas`\n",
    "# we know `location_wiki` is a dict\n",
    "type(location_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so let's use pandas' DataFrame attribute `from_dict` and orient our keys so that they are the index instead of columns\n",
    "df = pd.DataFrame.from_dict(location_wiki, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a peek at df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have an index with the names of our locations...\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we have 3 columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe our columns\n",
    "df.describe(include='all') # the `include` parameter allows us to choose what kinds of data types to include\n",
    "# `describe` only includes numeric columns by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can see that columns 0 and 2 have only one unique value ('html' for 0, and an empty string for 2)\n",
    "# let's drop these two columns and save over df using `inplace`\n",
    "df.drop(columns=[\n",
    "    0,\n",
    "    2\n",
    "], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what df looks like now\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while we only have one column in df, the name of the column isn't very informative. Let's rename it and save over df\n",
    "df.rename(columns={\n",
    "    1 : 'scraped'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the column name changed\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to begin disecting our scraped data, it would be nice to know how to look at one row\n",
    "# there are multiple ways to do this, but the best practice is to use label or integer indexing\n",
    "# label indexing uses the `.loc` property\n",
    "df.loc['Cancun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# integer indexing uses the `.iloc` property\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can get a list of rows by giving the property a list of the indices\n",
    "df.loc[[\n",
    "    'Cancun',\n",
    "    'Bahamas'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also give the property a \"slice\"\n",
    "df.iloc[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that only 2 rows were returned (the second and third)\n",
    "# This is becasue the slice is inclusive of the left, and exclusive of the right\n",
    "# If you want to start from an index and include everything after, you can leave of the right bound\n",
    "df.iloc[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also index on columns\n",
    "# if you want all rows and a specific column you use the colon (:) for the rows, then a comma to begin indexing on columns\n",
    "# remember that the colon is used for \"slicing\"\n",
    "df.loc[:, 'scraped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you index on one column, a `series` is returned, which is just data analysis jargon for column\n",
    "type(df.loc[:, 'scraped'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we've figured out how to slice our data, let's look at one row and disect the html in the 'scraped' column\n",
    "cancun_scraped = df.loc['Cancun', 'scraped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of the value\n",
    "type(cancun_scraped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the value is soup!\n",
    "# this means we it has attributes that we can use to extract values from the html\n",
    "# looking at the webpage for this index, we can see that there is a Table of Contents (toc), find it in the html/store it\n",
    "cancun_scraped_toc = cancun_scraped.find(name='div', attrs={\n",
    "    'id' : 'toc',\n",
    "    'class' : 'toc'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we've got the toc, but how do we get the values?\n",
    "# notice in the html the values \"ul\" and \"li\"\n",
    "# \"ul\" means \"unordered list\" and \"li\" means \"list item\"\n",
    "# we need the whole \"ul\" and each \"li\" in it\n",
    "# to get to the \"ul\" quickly, we can actually use dot-notation to access it, `find_all` the \"li\" in it, store it\n",
    "cancun_scraped_toc_ul = cancun_scraped_toc.ul.find_all(name='li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first \"li\" in the \"ul\"\n",
    "cancun_scraped_toc_ul[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this \"li\" has a few bits of information\n",
    "# but we're only interested in the Table of Contents Text (toctext)\n",
    "# extract it for this \"li\", then look at the text\n",
    "cancun_scraped_toc_ul[0].find(name='span', attrs={\n",
    "    'class' : 'toctext'\n",
    "}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the goal here is to do this for every \"li\"\n",
    "# let's test it on our current \"ul\"\n",
    "for li in cancun_scraped_toc_ul:\n",
    "    print(li.find(name='span', attrs={\n",
    "        'class' : 'toctext'\n",
    "    }).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it worked\n",
    "# to do this for all our rows we can `apply` our current work flow to df\n",
    "# start by `find`ing the \"toc\" and saving the returned html as a new column, \"table_of_contents\"\n",
    "df['table_of_contents'] = df.scraped.apply(lambda x : x.find(name='div', attrs={\n",
    "    'id' : 'toc',\n",
    "    'class' : 'toc'\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at our new column\n",
    "df.table_of_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that value in the \"South Padre Island\" row is None\n",
    "# this should mean that the wiki page for South Padre Island has not Table of Contents\n",
    "# let's check and make sure that's true... it is\n",
    "# if you were to search \"South Padre Island, Texas\" you get a Table of Contents, but that's not what we searched\n",
    "# let's continue applying our workflow to our new `table_of_contents` column\n",
    "df.table_of_contents.apply(lambda x : x.ul.find_all(name='li'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to look for \"ul\" returns an AttributeError\n",
    "# this is because \"South Padre Island\" doesn't have the attribute \"ul\" (because it doesn't have a \"toc\")\n",
    "# to get around this we can `.dropna` from \"table_of_contents\" before applying our function\n",
    "# do this, then store the results in a new column \"toc_ul_li\"\n",
    "df['toc_ul_li'] = df.table_of_contents.dropna().apply(lambda x : x.ul.find_all(name='li'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the column\n",
    "df.toc_ul_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we now have lists for values in our toc_ul_li column\n",
    "# to apply our function on each item in the list we'll need to use list comprehension\n",
    "# also, don't forget to `.dropna` else risk an error\n",
    "# when you've figured it out, store as \"toc_text\"\n",
    "df['toc_text'] = df.toc_ul_li.dropna().apply(lambda x : [li.find(name='span', attrs={\n",
    "    'class' : 'toctext'\n",
    "}).text for li in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the column \"toc_text\"\n",
    "df.toc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we still have lists as our values... but at least they are lists of text and not html\n",
    "# before we continue any further let's drop some columns that we don't need anymore\n",
    "# first, list the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'scraped' contains all the html for each row; it should be kept\n",
    "# 'table_of_contents' contains the original html for the toc; it can be dropped\n",
    "# 'toc_ul_li' contains a list of the html for each item in the toc; it can be dropped\n",
    "# 'toc_text' is what we were trying to get to; it should be kept and probably renamed to 'toc\n",
    "# drop the columns\n",
    "df.drop(columns=[\n",
    "    'table_of_contents',\n",
    "    'toc_ul_li'\n",
    "], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename 'toc_text' to 'toc'\n",
    "df.rename(columns={\n",
    "    'toc_text' : 'toc'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at what df looks like now\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nice and clean\n",
    "# let's get some general information about each row from our toc column, like how many items are in each\n",
    "# to do this we'll look at the toc column only, and then find the `len`gth of the list in each row\n",
    "df.toc.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the toc's lengths look different\n",
    "# let's visualize this in a bar graph\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "df.toc.str.len().plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the bar plot makes it easy to compare the toc lengths amongst the locations\n",
    "# however, it's still hard to see the ascending/descending length order\n",
    "# let's sort the values and then graph again\n",
    "df.toc.str.len().sort_values().plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that \"South Padre Island\" is at the far right rather than the left\n",
    "# this is because the value is `NaN` rather than 0. If it was 0 then it would be on the far left\n",
    "# suppose you wanted to graph the values in descending order rather than ascending\n",
    "# `.sort_values` accepts the parameter `ascending` where the default value is `True`\n",
    "# switch it to `False` and watch the graph reverse\n",
    "df.toc.str.len().sort_values(ascending=False).plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that \"South Padre Island\" was placed on the far right again\n",
    "# `.sort_values` does this by default with the parameter `na_position` which is set to \"last\" by default\n",
    "# suppose we want a row for each toc item with respect to location\n",
    "# to do this we can apply the function `pd.Series` to the column and watch the values explode into columns\n",
    "df.toc.apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store this as toc_explode so we can work with it separate from df\n",
    "toc_explode = df.toc.apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the columns\n",
    "toc_explode.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# there are 56 of them (range between 0 and 56, inclusive)\n",
    "# looking at the bar plot (or table) above we can see the Dominican Republic has 57 items in its toc\n",
    "# since it is the largest, that was the number of columns made\n",
    "# if a location doesn't have that many, it fills in as many columns as it can with its values, and NaN for the rest\n",
    "# the issue with this is we want rows, not columns\n",
    "# we also want to keep our locations in the index\n",
    "# to do this manipulation we'll have to `melt` our dataframe\n",
    "toc_explode.melt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply performing the `melt` causes us to lose information (*hint, hint, the index*)\n",
    "# to keep the index, we'll reset it to a column first\n",
    "toc_explode.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at what we did\n",
    "toc_explode.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now let's perform the melt and use the parameter `id_vars`\n",
    "toc_explode.melt(id_vars='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the \"index\" column to `id_vars` allows it to remain a column\n",
    "# we can also use the parameter `value_name` to name the column full of table of contents values\n",
    "# store the result as `toc_melt`\n",
    "toc_melt = toc_explode.melt(id_vars='index', value_name='toc_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what it looks like\n",
    "toc_melt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't need the column \"variable\"; drop it\n",
    "toc_melt.drop(columns='variable', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check it out again\n",
    "toc_melt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to return the column \"index\" to our index\n",
    "toc_melt.set_index('index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at it\n",
    "toc_melt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rememeber the index of our original df has no name, but our current one does (\"index\")\n",
    "# rename the index name as an empty string\n",
    "toc_melt.index.name = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at what happened\n",
    "toc_melt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# good\n",
    "# if you look at more of the dataframe you can see NaN values\n",
    "toc_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these came from instances where the location index didn't have that many columns\n",
    "# they don't give us any information beyond that so we can drop them\n",
    "toc_melt.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look at the dataframe now\n",
    "toc_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the melted data back with our original df and store\n",
    "df = df.join(toc_melt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look what happened\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we still have the \"toc\" column, but we don't really need it anymore because we have the \"toc_values\" column\n",
    "# let's drop the \"toc\" column\n",
    "df.drop(columns='toc', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we dropped the \"toc\" column we have to find a new way to count the number of \"toc_values\"\n",
    "# to do these we just need to `groupby` our index and `count` our \"toc_values\"\n",
    "df.groupby(level=0).toc_values.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voila!\n",
    "# we get the same results, and we can graph it the same way too\n",
    "df.groupby(level=0).toc_values.count().sort_values().plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the cool thing about extracting the toc_values is that we can now use the column values independently\n",
    "# for example, let's group on the \"toc_values\" column and count the number of locations that share them\n",
    "# we know that each location has exactly one unique scraped value, so we'll use it as our column for aggregation\n",
    "df.groupby('toc_values').scraped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a quick look at the top few rows and we can see 3 locations share the toc_value of \"Administrative Divisions\"\n",
    "# lets `sort_values` in descending and see what else is shared\n",
    "df.groupby('toc_values').scraped.count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some interesting stuff here\n",
    "# we can go even deeper by grouping on our share counts and then counting the number of toc_values in each\n",
    "df.groupby('toc_values').scraped.count().reset_index().groupby('scraped').toc_values.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this table shows us that there are 147 toc_values that belong to exactly one location, 14 that belong to two, etc.\n",
    "# let's graph this and add some labels so we know what's going on\n",
    "# create the graph, add a `title` parameter, store as `ax`\n",
    "ax = df.groupby('toc_values').scraped.count().reset_index().groupby('scraped').toc_values.count().plot.bar(title='toc_values shared by locations')\n",
    "# set the x-label\n",
    "ax.set_xlabel('number of locations')\n",
    "# set the y-label\n",
    "ax.set_ylabel('number of toc_values');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's see which 4 toc_values belong to 7 locations\n",
    "df.groupby('toc_values').scraped.count().where(lambda x : x == 7).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# it's important to note that there is more than one way to do things in python\n",
    "# but according to the \"Zen of Python\":\n",
    "# 'There should be one-- and preferably only one --obvious way to do it.\n",
    "# Although that way may not be obvious at first unless you're Dutch.'\n",
    "import this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I'm not Dutch\n",
    "# so for a long time my go to solution for finding the toc_values shared amongst 7 locations would have been the following\n",
    "test = df.groupby('toc_values').scraped.count()\n",
    "test.loc[test == 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obviously this solution works, but the downsides are:\n",
    "# 1) I had to define a new variable which sucks up memory\n",
    "# 2) defining a new variable is pointless if all you're going to do is filter it to see something\n",
    "# hence the `.where` method is a much better solution\n",
    "# getting back to the toc_values, a couple of these aren't much use to us right now (\"External Links\", \"See also\")\n",
    "# \"Education\" and \"History\" sound intersting, and since 7/8 of the locations share it, we can probably make columns for them\n",
    "# let's limits our \"scraped\" to just the Bahamas and find the \"education\" section\n",
    "bahamas_scraped = df.loc['Bahamas', 'scraped'].unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the html we can see that the \"Education\" section is marked by 'span' with attributes 'class' and 'id'\n",
    "bahamas_scraped_education_span = bahamas_scraped.find(name='span', attrs={\n",
    "    'class' : 'mw-headline',\n",
    "    'id' : 'Education'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/42450743/extracting-the-text-between-two-header-tags-using-beautifulsoup-in-python\n",
    "\n",
    "# what I want to do is pull all of the tags after the \"Education\" header, but before the next header\n",
    "# to do this we will reference the 'span' tag,\n",
    "# take a step back to its parent (<h2>),\n",
    "# then use the method `.nextSiblingGenerator` to run through the siblings until we see another <h2> tag\n",
    "# store the results in a list\n",
    "# define the empty list...\n",
    "education = []\n",
    "# loop through the parent siblings...\n",
    "for i in bahamas_scraped_education_span.parent.nextSiblingGenerator():\n",
    "    # check the sibling name isn't \"h2\"...\n",
    "    if i.name == 'h2':\n",
    "        break\n",
    "    # append the sibling...\n",
    "    else:\n",
    "        education.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at our education list\n",
    "education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the 3 '\\n' in our list?\n",
    "# we don't want those as they don't provide any information\n",
    "# to this we can check to see what their type is compared to the other items in our list\n",
    "[type(i) for i in education]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the '\\n' have type `bs4.element.NavigableString` and the other items have type `bs4.element.Tag`\n",
    "# to keep the NavigableStrings from getting in our list we can filter them out with an `if` statement\n",
    "# but first we need to import `element` from `bs4`\n",
    "from bs4 import element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education = []\n",
    "for i in bahamas_scraped_education_span.parent.nextSiblingGenerator():\n",
    "    if i.name == 'h2':\n",
    "        break\n",
    "    else:\n",
    "        # filter out the `NavigableString`\n",
    "        if type(i) != element.NavigableString:\n",
    "            education.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at our list now\n",
    "education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perfect! let's repeat this for all other locations and store the results in an \"education_html\" series\n",
    "education_html = df.scraped.drop_duplicates().apply(lambda x : x.find(name='span', attrs={\n",
    "    'class' : 'mw-headline',\n",
    "    'id' : 'Education'\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nulls\n",
    "education_html.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since there are nulls, we will have to remember to dropna later\n",
    "# something else to note is that we cannot use a `break` inside a list-comprehension\n",
    "# there are methods for getting around this while still doing a \"one-liner\", but they are beyond the scope of this tutorial\n",
    "# this means we will have to define our own function and then apply it to our series\n",
    "# inside our function we will place our for-loop code almost as-is\n",
    "def education(x):\n",
    "    education = []\n",
    "    for i in x.parent.nextSiblingGenerator():\n",
    "        if i.name == 'h2':\n",
    "            break\n",
    "        else:\n",
    "            if type(i) != element.NavigableString:\n",
    "                education.append(i)\n",
    "    return education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# there are two ways to apply our function\n",
    "# the first way is to use a lambda expression like we've been doing\n",
    "education_html.dropna().apply(lambda x : education(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the second way is just to apply the function\n",
    "education_html.dropna().apply(education)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the reason the second way works is because when you are applying a function,\n",
    "# lambda is only needed when you want to do several things to the `x`\n",
    "# if you are doing just one thing (i.e. applying one function), you can use the function as the only parameter\n",
    "# let's convert our series `to_frame` and name it \"education\", then join it to `df`\n",
    "df = df.join(education_html.dropna().apply(education).to_frame(name='education'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we now have an education column we can remove the rows in our \"toc_values\" where it says \"Education\"\n",
    "df = df.loc[df.toc_values != 'Education'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise: repeat the above tasks for \"History\"\n",
    "df = df.join(df.scraped.drop_duplicates().apply(lambda x : x.find(name='span', attrs={\n",
    "    'class' : 'mw-headline',\n",
    "    'id' : 'History'\n",
    "})).dropna().apply(education).to_frame(name='history')).loc[df.toc_values != 'History'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have two new columns, we can analyze them\n",
    "# let's see how many tags are in the education column with respect to location\n",
    "df.groupby(level=0).education.apply(lambda x : x.str.len()).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also look at how many tags are in the history column\n",
    "df.groupby(level=0).history.apply(lambda x : x.str.len()).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do them at the same time\n",
    "df.groupby(level=0)[[\n",
    "    'education',\n",
    "    'history'\n",
    "]].transform(lambda x : x.str.len()).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that we used the method `transform` rather than `apply`\n",
    "# if we had used apply we would have received an AttributeError\n",
    "# this is because the apply function is mean for series\n",
    "# what we have created before the transform is a `groupby` object, and the transform returns a dataframe\n",
    "# now let's plot our results\n",
    "df.groupby(level=0)[[\n",
    "    'education',\n",
    "    'history'\n",
    "]].transform(lambda x : x.str.len()).drop_duplicates().plot.bar(figsize=(10,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.melt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's clear that the Dominican Republic has more tags in the \"History\" section of the wiki\n",
    "# but Miami has the more tags in the \"Education\" section\n",
    "# let's dig a little further and see what kinds of tags are in each section starting with the \"education\" column\n",
    "df.education.dropna().apply(lambda x : [i.name for i in x]).apply(pd.Series).drop_duplicates().reset_index().melt(id_vars='index', value_name='education_tags').drop(columns='variable').rename(columns={\n",
    "    'index' : ''\n",
    "}).set_index('').dropna().groupby(level=0).education_tags.agg([\n",
    "    'count',\n",
    "    pd.Series.nunique\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
